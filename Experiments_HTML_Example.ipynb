{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 Build Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 28/572 [00:00<00:06, 89.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Company: Aetna IncDate: 2017-01-31\n",
      "Error Company: Aetna IncDate: 2017-05-02\n",
      "Error Company: Aetna IncDate: 2017-10-31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 110/572 [00:01<00:04, 114.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Company: CA, Inc.Date: 2017-08-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 559/572 [00:04<00:00, 109.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Company: Vulcan MaterialsDate: 2017-08-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 572/572 [00:04<00:00, 117.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.237429941531154\n",
      "563 563 563\n",
      "563\n"
     ]
    }
   ],
   "source": [
    "import tools\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "def make_sorted_list_and_save(days):\n",
    "    pickle_in = open(\"stock_price_dict.pickle\",\"rb\")\n",
    "    price_dict_ = pickle.load(pickle_in)\n",
    "    #DATA_DIR = 'D:\\Dataset\\EarningsCallData\\ReleasedDataset\\ReleasedDataset_mp3'\n",
    "    DATA_DIR = '.\\EarningsCallData\\ACL19_Release\\ACL19_Release'\n",
    "    stock_movement_3days, text_all, date, company = tools.calculate_movement(price_dict_, days, DATA_DIR)\n",
    "    print(len(stock_movement_3days), len(text_all), len(date))\n",
    "\n",
    "    combined_list = list(zip(date, stock_movement_3days, company, text_all)) #进行数据的聚合、组合或并行处理。\n",
    "    print(len(combined_list))\n",
    "    sorted_list = sorted(combined_list, key=lambda x: x[0])#按照列表中元素的第一个索引（日期）进行排序\n",
    "\n",
    "    np.save(f'sorted_list_{days}days.npy', sorted_list)\n",
    "\n",
    "def make_and_save(days):\n",
    "    make_sorted_list_and_save(days)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print()\n",
    "    make_and_save(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Part 2 Token-Level Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 563/563 [09:12<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(563, 512, 1024)\n",
      "563\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from tqdm import tqdm #用于在循环迭代中显示进度条的库\n",
    "import torch\n",
    "\n",
    "#Generate sentence representation 使用预训练的语言模型来计算文本的嵌入表示\n",
    "def emb_str(text):\n",
    "    input_ids = torch.tensor([tokenizer.encode(text)])[:,:512] #使用 tokenizer 对文本进行编码，将其转换为输入张量 input_ids。编码后的张量可能会被裁剪为最大长度512。\n",
    "    # print(input_ids.shape)\n",
    "    with torch.no_grad(): #是一个上下文管理器，用于在执行期间禁用梯度计算。在这个上下文中，所有的张量操作都不会被跟踪，也不会导致梯度被计算和存储。这在进行推断或评估时非常有用，因为在这些情况下通常不需要计算梯度。\n",
    "        last_hidden_states = model(input_ids)[0]  # Models outputs are now tuples 使用模型对输入张量进行前向传递，获得最后一层隐藏状态 只取第一个元素\n",
    "        emb = last_hidden_states.cpu().numpy() #将最后一层隐藏状态转移到 CPU 上，并将其转换为 NumPy 数组\n",
    "    return emb\n",
    "\n",
    "# Padding\n",
    "def make(days,name):\n",
    "    list = np.load(f\"{name}_list_{days}days.npy\")\n",
    "    embs = []\n",
    "    labels = []\n",
    "    sentence_len = []\n",
    "    for i in tqdm(range(len(list))):\n",
    "        # print(list[i])\n",
    "        text = (list[i][2] + \"\\n\" + list[i][3]).split(\"\\n\")#company \\n text contnent\n",
    "        sentence_len.append(len(text))\n",
    "        text_embs = emb_str(text)\n",
    "        text_embs = np.squeeze(text_embs)#将 text_embs 中的单维度条目进行删除，以便在后续的处理中更方便地使用\n",
    "        # text_embs = np.concatenate((text_embs, np.array([np.array(past_volatility_all[i])] * len(text_embs))), axis=1)\n",
    "        embs.append(text_embs)\n",
    "        labels.append(float(list[i][1]))#movement value->labels\n",
    "\n",
    "    b = np.zeros([len(embs),len(max(embs,key = lambda x: len(x))),1024])#创建一个全零数组 b，形状为 (len(embs), max_sentence_len, 1024)，用于存储嵌入向量，并将嵌入向量复制到 b 中对应的位置\n",
    "    for i,j in enumerate(embs):#使用 enumerate(embs) 遍历嵌入向量列表 embs，其中 i 是索引，j 是对应的嵌入向量\n",
    "        b[i][0:len(j),:] = j #将嵌入向量 j 复制到数组 b 的对应位置。这里使用了切片操作 0:len(j) 来确保复制的部分与嵌入向量的长度一致\n",
    "    print(b.shape)\n",
    "    print(len(labels))\n",
    "\n",
    "    np.save(f\"{name}_embed_{days}days.npy\", b)\n",
    "    np.save(f\"{name}_label_{days}days.npy\", labels)\n",
    "\n",
    "def make_emb(days):\n",
    "    make(days, \"sorted\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print()\n",
    "    model = transformers.BertModel.from_pretrained('./bert-large-uncased-whole-word-masking')#通过加载预训练的BERT模型来创建一个BERT模型实例\n",
    "    tokenizer = transformers.BertTokenizer.from_pretrained('./bert-large-uncased-whole-word-masking')\n",
    "    make_emb(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 Customized Sentence-level Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import random, math"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example for single task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#给定的批量矩阵中进行遮蔽操作\n",
    "def mask_(matrices, maskval=0.0, mask_diagonal=True):\n",
    "    \"\"\"\n",
    "    Masks out all values in the given batch of matrices where i <= j holds,\n",
    "    i < j if mask_diagonal is false\n",
    "\n",
    "    In place operation\n",
    "\n",
    "    :param tns:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    #在遮蔽过程中，矩阵中满足条件 i <= j（如果mask_diagonal为True）或 i < j（如果mask_diagonal为False）的元素将被遮蔽（置为指定的maskval值）。\n",
    "    b, h, w = matrices.size()#函数的输入参数matrices是一个大小为 (batch_size, height, width) 的张量，表示一批矩阵。maskval参数是要用于遮蔽的值，默认为0.0。mask_diagonal参数控制是否遮蔽对角线上的元素，默认为True，即遮蔽对角线上的元素\n",
    "\n",
    "    indices = torch.triu_indices(h, w, offset=0 if mask_diagonal else 1)#生成一个大小为 (2, num_indices) 的张量，其中 num_indices 是根据矩阵大小和遮蔽对角线的设置计算得到的索引数量。这个张量包含了上三角部分（包括或不包括对角线）的索引\n",
    "    matrices[:, indices[0], indices[1]] = maskval #将遮蔽值 maskval 赋值给 matrices 张量中对应索引的位置。通过这样的操作，上三角部分的元素将被遮蔽（置为指定的遮蔽值），而下三角部分的元素将保持不变\n",
    "\n",
    "\n",
    "\n",
    "def contains_nan(tensor):\n",
    "    return bool((tensor != tensor).sum() > 0)#检查张量中是否包含NaN\n",
    "\n",
    "\n",
    "\n",
    "# Self-Attention 自注意力机制是用于捕捉序列数据中不同位置之间的关联关系\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, emb, heads=8, mask=False):\n",
    "        \"\"\"\n",
    "        :param emb:\n",
    "        :param heads:\n",
    "        :param mask:\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()#表示调用 nn.Module 类的构造函数，用于初始化继承自 nn.Module 的子类的基本功能\n",
    "\n",
    "        self.emb = emb\n",
    "        self.heads = heads\n",
    "        self.mask = mask\n",
    "\n",
    "        self.tokeys = nn.Linear(emb, emb * heads, bias=False)\n",
    "        self.toqueries = nn.Linear(emb, emb * heads, bias=False)\n",
    "        self.tovalues = nn.Linear(emb, emb * heads, bias=False)#将输入 x 映射到键（keys）、查询（queries）和值（values）空间的线性变换\n",
    "\n",
    "        self.unifyheads = nn.Linear(heads * emb, emb)#heads * emb 表示输入特征的大小，emb 表示输出特征的大小。\n",
    "\n",
    "    def forward(self, x):#输入 x 的形状为 (b, t, e)，其中 b 是批量大小，t 是序列长度，e 是嵌入维度。首先，通过线性变换将输入分别映射到键、查询和值的空间，并将维度进行调整，使其适应自注意力计算的需求\n",
    "\n",
    "        b, t, e = x.size()\n",
    "        h = self.heads#在 SelfAttention 类的前向传播函数中，h = self.heads 是将类的 self.heads 属性赋值给局部变量 h。self.heads 表示模型中的注意力头数，而 h 则是用于表示头数的变量\n",
    "        assert e == self.emb# 断言语句，用于检查变量 e 是否等于模型的 self.emb 属性. 如果断言条件为 False，则会触发断言错误，并抛出异常。\n",
    "\n",
    "        keys    = self.tokeys(x)   .view(b, t, h, e)\n",
    "        queries = self.toqueries(x).view(b, t, h, e)\n",
    "        values  = self.tovalues(x) .view(b, t, h, e)#self.tovalues(x) 表示将输入张量 x 经过线性变换 self.tovalues，得到值表示，形状为 (b, t, h * e). b 表示批量大小，t 表示序列长度，h 表示注意力头数，e 表示特征维度。通过变换操作，将输入张量的特征维度转换为 (h * e)，并在维度上扩展为 (b, t, h, e) 的形状，以便后续的自注意力计算\n",
    "\n",
    "        # compute scaled dot-product self-attention\n",
    "\n",
    "        # - fold heads into the batch dimension\n",
    "        keys = keys.transpose(1, 2).contiguous().view(b * h, t, e)\n",
    "        queries = queries.transpose(1, 2).contiguous().view(b * h, t, e)\n",
    "        values = values.transpose(1, 2).contiguous().view(b * h, t, e)\n",
    "        #首先，使用 transpose 将维度 1 和维度 2 进行转置，以便在后续计算中，序列长度 t 可以成为注意力矩阵的行数，而特征维度 e 可以成为注意力矩阵的列数。然后，使用 contiguous 确保张量在内存中是连续存储的\n",
    "        #使用 view 将形状为 (b, h, t, e) 的张量重塑为形状为 (b * h, t, e) 的张量。这样做是为了将注意力头数 h 与批量大小 b 进行合并，形成一个新的批量维度\n",
    "        queries = queries / (e ** (1/4))\n",
    "        keys    = keys / (e ** (1/4))#对 queries 和 keys 进行缩放，除以 (e ** (1/4))，这是为了缩放注意力矩阵的值，使得在计算注意力概率时更稳定。\n",
    "        # - Instead of dividing the dot products by sqrt(e), we scale the keys and values.\n",
    "        #   This should be more memory efficient\n",
    "\n",
    "        # - get dot product of queries and keys, and scale\n",
    "        dot = torch.bmm(queries, keys.transpose(1, 2))#使用批量矩阵乘法 torch.bmm 计算查询向量和键向量之间的点积.点积操作会将查询向量的每个查询与键向量的每个键进行相乘，并将结果按键的维度进行求和。这将产生一个注意力矩阵，其形状为 (b * h, t, t)，其中每个元素表示查询与键之间的相似度或相关性\n",
    "\n",
    "        assert dot.size() == (b*h, t, t)\n",
    "\n",
    "        if self.mask: # mask out the lower half of the dot matrix,including the diagonal\n",
    "            mask_(dot, maskval=float('-inf'), mask_diagonal=False) #如果 self.mask 为真，将屏蔽掉 dot 矩阵的下三角部分，包括对角线。mask_() 函数被用于执行屏蔽操作。\n",
    "\n",
    "        dot = F.softmax(dot, dim=2) # dot now has row-wise self-attention probabilities 通过在维度2上对 dot 进行 softmax 操作，得到行级别的自注意力概率\n",
    "\n",
    "        assert not contains_nan(dot[:, 1:, :]) # only the forst row may contain nan\n",
    "\n",
    "        if self.mask == 'first':# 如果 self.mask 为 'first'，则将 dot 的第一行克隆为新的张量，并将其置为零。这是为了处理在 softmax 操作中可能导致除以零的情况，从而避免产生 NaN 值\n",
    "            dot = dot.clone()\n",
    "            dot[:, :1, :] = 0.0\n",
    "            # - The first row of the first attention matrix is entirely masked out, so the softmax operation results\n",
    "            #   in a division by zero. We set this row to zero by hand to get rid of the NaNs\n",
    "\n",
    "        # apply the self attention to the values\n",
    "        out = torch.bmm(dot, values).view(b, h, t, e) #自注意力概率矩阵 dot 与值矩阵 values 执行矩阵乘法，得到经过自注意力机制处理后的输出\n",
    "\n",
    "        # swap h, t back, unify heads\n",
    "        out = out.transpose(1, 2).contiguous().view(b, t, h * e) #通过交换 h 和 t 的顺序，并将头数 h 与特征维度 e 相乘，将输出 out 的形状转换为 (b, t, h * e)\n",
    "\n",
    "        return self.unifyheads(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer Block\n",
    "\n",
    "class TransformerBlock(nn.Module):#Transformer Block 是 Transformer 模型的基本构建块之一，它由自注意力层、前馈神经网络层和残差连接组成。\n",
    "    def __init__(self, emb, heads, mask, seq_length, ff_hidden_mult=4, dropout=0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention = SelfAttention(emb, heads=heads, mask=mask)#自注意力层，用于对输入进行自注意力计算\n",
    "        self.mask = mask\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(emb)#两个 Layer Normalization 层，用于对自注意力层输出和前馈神经网络层输出进行归一化\n",
    "        self.norm2 = nn.LayerNorm(emb)\n",
    "\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(emb, ff_hidden_mult * emb),#线性变换层，它将输入的特征维度 emb 转换为 ff_hidden_mult * emb 的维度。\n",
    "            nn.ReLU(), #这是一个 ReLU 激活函数，它对线性变换的输出进行非线性变换，引入非线性性质\n",
    "            nn.Linear(ff_hidden_mult * emb, emb) #另一个线性变换层，它将前一层的输出维度 ff_hidden_mult * emb 转换回原始的特征维度 emb\n",
    "        )#self.ff 是一个 nn.Sequential 对象，它定义了一个前馈神经网络层。这个前馈神经网络层由两个线性变换层和一个 ReLU 激活函数组成。通过这个前馈神经网络层，模型可以对输入的特征进行非线性变换和映射。这有助于模型学习更复杂的特征表示和抽象\n",
    "        #因为在 Transformer 模型中的每个编码层中，存在一个前馈神经网络（Feed-Forward Neural Network）层，用于引入非线性性质和增加模型的表示能力。该前馈神经网络层通常由两个线性变换（nn.Linear）和一个非线性激活函数（如 ReLU）组成。\n",
    "        #在该前馈神经网络层中，将输入特征的维度从 emb 转换为 ff_hidden_mult * emb，其中 ff_hidden_mult 是一个超参数，通常设置为大于 1 的值。这样做的目的是通过引入更高维度的表示空间和非线性变换，使模型能够学习更复杂的特征和模式。\n",
    "        #在前馈神经网络层后面，通过另一个线性变换将维度从 ff_hidden_mult * emb 转换回原始的特征维度 emb。这样做是为了保持编码层的输出与输入的维度一致，以便能够进行残差连接（residual connection），从而更好地传递和保留原始输入的信息。\n",
    "        #通过引入非线性性质和通过线性变换改变特征维度的方式，前馈神经网络层在编码层中增加了模型的非线性表示能力，同时保持了输入和输出的一致性，有助于模型进行更高级的特征提取和表示学习。\n",
    "        \n",
    "        self.do = nn.Dropout(dropout)# Dropout 层，用于在训练过程中进行随机失活以防止过拟合。\n",
    "\n",
    "    def forward(self, x):#定义了 TransformerBlock 的前向传播过程。\n",
    "\n",
    "        attended = self.attention(x) #将输入 x 传递给自注意力层（self.attention），以计算自注意力概率矩阵\n",
    "\n",
    "        x = self.norm1(attended + x) #将自注意力层的输出（attended）与输入 x 相加，并应用层归一化（self.norm1）来得到规范化后的输出\n",
    "\n",
    "        x = self.do(x) #应用丢弃（dropout）操作（self.do）来对输出进行正则化，防止过拟合\n",
    "\n",
    "        fedforward = self.ff(x)#将前一步骤的输出 x 传递给前馈神经网络层（self.ff），进行非线性变换\n",
    "\n",
    "        x = self.norm2(fedforward + x)#将前馈神经网络层的输出（fedforward）与之前的输出 x 相加，并再次应用层归一化\n",
    "\n",
    "        x = self.do(x)\n",
    "\n",
    "        return x\n",
    "    #通过以上步骤，TransformerBlock 实现了一个 Transformer 模型中的一个编码层，其中包括了自注意力机制、前馈神经网络和残差连接。这些操作有助于模型进行特征提取和表示学习，从而实现对输入序列的编码\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##RTransformer 序列回归\n",
    "\n",
    "class RTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer for sequences Regression    \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, emb, heads, depth, seq_length, num_tokens, num_classes, max_pool=True, dropout=0.0):\n",
    "        \"\"\"\n",
    "        :param emb: Embedding dimension\n",
    "        :param heads: nr. of attention heads\n",
    "        :param depth: Number of transformer blocks\n",
    "        :param seq_length: Expected maximum sequence length\n",
    "        :param num_tokens: Number of tokens (usually words) in the vocabulary\n",
    "        :param num_classes: Number of classes.\n",
    "        :param max_pool: If true, use global max pooling in the last layer. If false, use global\n",
    "                         average pooling.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_tokens, self.max_pool = num_tokens, max_pool\n",
    "\n",
    "        #self.token_embedding = nn.Embedding(embedding_dim=emb, num_embeddings=num_tokens) 词嵌入层（Token Embedding）：该层将输入的词索引映射到连续的向量表示，即词嵌入。这种嵌入通常使用一个 nn.Embedding 层实现，其中 embedding_dim 参数指定了词嵌入的维度，而 num_embeddings 参数表示词汇表中的单词数量。\n",
    "        self.pos_embedding = nn.Embedding(embedding_dim=emb, num_embeddings=seq_length)#位置嵌入层（Position Embedding）：由于 Transformer 模型没有显式的位置信息，为了引入序列中每个位置的相对位置信息，使用了位置嵌入。该层将输入序列的位置索引映射为相应的位置嵌入向量。在这个模型中，使用了一个 nn.Embedding 层来实现位置嵌入，其中 embedding_dim 参数指定了位置嵌入的维度，而 num_embeddings 参数表示序列的最大长度。\n",
    "\n",
    "        tblocks = []#在这段代码中，通过一个循环，将多个 TransformerBlock 实例化并添加到 tblocks 列表中。这个循环的次数由参数 depth 控制，表示模型中 TransformerBlock 的层数\n",
    "        for i in range(depth):\n",
    "            tblocks.append(\n",
    "                TransformerBlock(emb=emb, heads=heads, seq_length=seq_length, mask=False, dropout=dropout))\n",
    "\n",
    "        self.tblocks = nn.Sequential(*tblocks)#通过 nn.Sequential 将 tblocks 列表中的多个 TransformerBlock 组合成一个串行的模型结构，即 self.tblocks。这样做可以方便地将输入按顺序通过所有的 TransformerBlock 进行处理。\n",
    "\n",
    "        self.toprobs = nn.Linear(emb, num_classes)#通过 nn.Linear 层（self.toprobs）将最后一个 TransformerBlock 的输出映射到预测的目标类别数量（num_classes）。这个线性层将输出的特征维度（emb）转换为目标类别的数量，以便进行后续的分类操作。\n",
    "\n",
    "        self.do = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: A batch by sequence length integer tensor of token indices.\n",
    "        :return: predicted log-probability vectors for each token based on the preceding tokens.\n",
    "        \"\"\"\n",
    "        #参数 x 是一个批次（batch）大小乘以序列长度的整数张量，表示输入的令牌索引。模型的目标是基于先前的令牌预测每个令牌的对数概率向量\n",
    "        sentences_emb = x#将输入 x 赋值给 sentences_emb 变量，表示输入的句子嵌入\n",
    "        b, t, e = x.size()\n",
    "\n",
    "        positions = self.pos_embedding(torch.arange(t))[None, :, :].expand(b, t, e)#对序列长度的范围进行嵌入，获取位置编码。位置编码的目的是为了在输入序列中引入每个令牌的位置信息。\n",
    "        #positions = torch.tensor(positions, dtype=torch.float32)\n",
    "        x = sentences_emb + positions#将位置编码添加到输入的句子嵌入中，以获得具有位置信息的句子表示\n",
    "        x = self.do(x)\n",
    "\n",
    "        x = self.tblocks(x)#每个 TransformerBlock 都会对输入序列进行自注意力计算和前馈神经网络操作，以获取更好的表示\n",
    "\n",
    "        x = x.max(dim=1)[0] if self.max_pool else x.mean(dim=1) # pool over the time dimension 对序列的时间维度进行池化操作，得到一个固定长度的向量表示。max_pool 池化操作选择最大值，mean_pool 池化操作选择平均值。\n",
    "        x = self.toprobs(x)#通过 self.toprobs 对输入 x 进行线性变换，将其映射到预测类别的空间。\n",
    "        x = torch.squeeze(x)#将张量 x 去除维度为1的尺寸，使其变为一维张量。这通常是为了消除不必要的尺寸，以便与预期的输出形状相匹配。在这里，x 是预测的对数概率向量，去除维度为1的尺寸后，得到的张量表示每个预测类别的对数概率。\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format Dataset\n",
    "\n",
    "from torch.utils import data\n",
    "\n",
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        'Initialization'\n",
    "        self.labels = labels\n",
    "        self.text = texts\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        if torch.is_tensor(index):\n",
    "            index = index.tolist()#您检查index是否是一个张量对象，如果是，您将其转换为Python列表。这是为了兼容处理张量索引和标量索引的情况。\n",
    "\n",
    "        # Load data and get label\n",
    "        X = self.text[index,:,:]\n",
    "        y = self.labels[index]\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Load your own the whole dataset\n",
    "TEXT_emb = np.load(\"sorted_embed_3days.npy\")\n",
    "LABEL_emb = np.load(\"sorted_label_3days.npy\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 Build the Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Main function\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import pandas as pd\n",
    "import random, sys, math, gzip\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "NUM_CLS = 1\n",
    "\n",
    "def go(arg):\n",
    "    \"\"\"\n",
    "    Creates and trains a basic transformer for any regression task.\n",
    "    \"\"\"\n",
    "\n",
    "    if arg.final:#arg.final 为真,按照 8:2 的比例划分为训练集和验证集。\n",
    "\n",
    "        train, val = train_test_split(TEXT_emb, test_size=0.2)\n",
    "        train_label, val_label = train_test_split(LABEL_emb, test_size=0.2)\n",
    "        training_set = Dataset(train, train_label)#使用 torch.utils.data.DataLoader 创建了训练集和验证集的数据加载器，用于批量加载数据进行训练和评估。\n",
    "        val_set = Dataset(val, val_label)\n",
    "\n",
    "    else:\n",
    "        train, val = train_test_split(TEXT_emb, test_size=0.2)\n",
    "        train_label, val_label = train_test_split(LABEL_emb, test_size=0.2)\n",
    "        train, val = train_test_split(train, test_size=0.2)#当 arg.final 为假时，即非最终情况，首先进行一次训练集和验证集的划分，然后再次对训练集和验证集进行划分的目的是为了创建更小的训练集和验证集.这种两次划分的方式可以帮助在较小的数据集上进行快速实验和调试，从而更高效地调整模型的超参数、模型结构等，以达到更好的性能和泛化能力\n",
    "        train_label, val_label = train_test_split(train_label, test_size=0.2)\n",
    "\n",
    "        training_set = Dataset(train, train_label)\n",
    "        val_set = Dataset(val, val_label)\n",
    "\n",
    "    trainloader=torch.utils.data.DataLoader(training_set, batch_size=arg.batch_size, shuffle=False, num_workers=0)#trainloader是训练集的数据加载器，它会从training_set中按照batch_size指定的批量大小取出数据进行训练。shuffle=False表示不对数据进行随机重排，num_workers=0表示在主进程中加载数据\n",
    "    testloader=torch.utils.data.DataLoader(val_set, batch_size=len(val_set), shuffle=False, num_workers=0)\n",
    "    print('training examples', len(training_set))\n",
    "    #print(f'- nr. of {\"test\" if arg.final else \"validation\"} examples {len(test_iter)}')\n",
    "\n",
    "    if arg.final:\n",
    "          print('test examples', len(val_set))\n",
    "    else:\n",
    "          print('validation examples', len(val_set))\n",
    "    #如果arg.final为True，则表示当前是最终的测试阶段，使用的是测试集。因此，打印语句print('test examples', len(val_set))输出测试集的样本数量。\n",
    "    #如果arg.final为False，则表示当前是验证阶段，使用的是验证集。因此，打印语句print('validation examples', len(val_set))输出验证集的样本数量。\n",
    "\n",
    "    # create the model\n",
    "    model = RTransformer(emb=arg.embedding_size, heads=arg.num_heads, depth=arg.depth, \\\n",
    "                         seq_length=arg.max_length, num_tokens=arg.vocab_size, num_classes=NUM_CLS, max_pool=arg.max_pool)\n",
    "    #     if torch.cuda.is_available():\n",
    "    #         model.cuda()\n",
    "\n",
    "    opt = torch.optim.Adam(lr=arg.lr, params=model.parameters())#使用Adam优化器进行参数优化，其中学习率由arg.lr指定\n",
    "\n",
    "    # training loop\n",
    "    seen = 0#记录已经处理的样本数\n",
    "    evaluation= {'epoch': [] ,'Train Accuracy': [], 'Test Accuracy' :[]}#epoch用于记录训练的轮数，Train Accuracy用于记录每轮训练后模型在训练集上的准确率，Test Accuracy用于记录每轮训练后模型在测试集上的准确率。\n",
    "    for e in tqdm_notebook(range(arg.num_epochs)):\n",
    "        train_loss_tol = 0.0\n",
    "        print('\\n epoch ' ,e)\n",
    "        model.train(True)#启用训练模式\n",
    "\n",
    "        for i, data in tqdm_notebook(enumerate(trainloader)):\n",
    "            if i > 2:\n",
    "                break\n",
    "            # learning rate warmup学习率预热\n",
    "            # - we linearly increase the learning rate from 10e-10 to arg.lr over the first\n",
    "            #   few thousand batches\n",
    "            if arg.lr_warmup > 0 and seen < arg.lr_warmup:\n",
    "                lr = max((arg.lr / arg.lr_warmup) * seen, 1e-10)\n",
    "                opt.lr = lr\n",
    "\n",
    "            opt.zero_grad()#清零优化器的梯度\n",
    "\n",
    "            inputs, labels = data\n",
    "            inputs = Variable(inputs.type(torch.FloatTensor))#将输入数据转换为FloatTensor类型，并将其封装在Variable中（用于自动求导）\n",
    "            # labels = torch.tensor(labels, dtype=torch.float32)\n",
    "            labels = labels.clone().detach()#克隆并分离标签，并将其转换为浮点型\n",
    "            if inputs.size(1) > arg.max_length:\n",
    "                inputs = inputs[:, :arg.max_length, :]#如果输入数据的长度超过了arg.max_length，则进行截断处理\n",
    "            out = model(inputs)\n",
    "            out = torch.unsqueeze(out, 0)\n",
    "            # print(out)\n",
    "            out = out.float()\n",
    "            labels = labels.float()\n",
    "\n",
    "            # print(out.shape,labels.shape)\n",
    "\n",
    "            loss = F.mse_loss(out, labels)#计算损失函数（均方误差损失）\n",
    "            train_loss_tol += loss#累加训练损失\n",
    "\n",
    "            loss.backward()#反向传播计算梯度\n",
    "\n",
    "            # clip gradients\n",
    "            # - If the total gradient vector has a length > 1, we clip it back down to 1.\n",
    "            if arg.gradient_clipping > 0.0:\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), arg.gradient_clipping)#调用nn.utils.clip_grad_norm_函数，传入模型的参数和梯度裁剪阈值作为参数，对模型的梯度进行裁剪。裁剪操作的目的是限制梯度的范数，以防止梯度爆炸的问题\n",
    "\n",
    "            opt.step()#更新模型的参数\n",
    "\n",
    "            seen += inputs.size(0)#更新迭代次数seen,增加当前批次的样本数量\n",
    "            # tbw.add_scalar('classification/train-loss', float(loss.item()), seen)\n",
    "        # print('train_loss: ',train_loss_tol)\n",
    "        train_loss_tol = train_loss_tol /( i +1)#计算训练损失的平均值\n",
    "        with torch.no_grad():#上下文管理器，表示接下来的计算不会对梯度产生影响\n",
    "\n",
    "            model.train(False)#进入评估模式\n",
    "            tot, cor= 0.0, 0.0\n",
    "\n",
    "            loss_test = 0.0#初始化测试集的损失loss_test为0.0\n",
    "            for i, data in tqdm_notebook(enumerate(testloader)):\n",
    "                if i > 2:\n",
    "                    break\n",
    "                inputs, labels = data\n",
    "                inputs, labels = torch.tensor(inputs, dtype=torch.float32), torch.tensor(labels, dtype=torch.float32)\n",
    "                if inputs.size(1) > arg.max_length:\n",
    "                    inputs = inputs[:, :arg.max_length, :]\n",
    "                out = model(inputs)\n",
    "\n",
    "                loss_test += F.mse_loss(out, labels)#计算预测结果与真实标签之间的均方误差损失，并累加到loss_test中\n",
    "                # tot = float(inputs.size(0))\n",
    "                # cor += float(labels.sum().item())\n",
    "\n",
    "            acc = loss_test.numpy()\n",
    "            if arg.final:\n",
    "                print('test accuracy', acc)\n",
    "            else:\n",
    "                print('validation accuracy', acc)#准确率计算实际上是损失值而不是准确率\n",
    "\n",
    "        #torch.save(model, './checkpoint/epoch' +str(e) +'.pth')\n",
    "        torch.save(model, '\\\\Users\\\\zc22364\\\\Downloads\\\\HTML_partial_reproduction-main\\\\HTML_partial_reproduction-main\\\\checkpoint\\\\epoch' +str(e) +'.pth')\n",
    "        # print(train_loss_tol)\n",
    "        # print(acc)\n",
    "        train_loss_tol = train_loss_tol.detach().numpy()\n",
    "        evaluation['epoch'].append(e)\n",
    "        evaluation['Train Accuracy'].append(train_loss_tol)#实际上存储的是训练集的损失值\n",
    "        evaluation['Test Accuracy'].append(acc)#测试集/验证集的准确率\n",
    "\n",
    "\n",
    "    evaluation = pd.DataFrame(evaluation)\n",
    "    evaluation.sort_values([\"Test Accuracy\"] ,ascending=True ,inplace=True)\n",
    "\n",
    "    return evaluation\n",
    "    # tbw.add_scalar('classification/test-loss', float(loss.item()), e)\n",
    "\n",
    "  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training examples 360\n",
      "validation examples 90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zc22364\\AppData\\Local\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel_launcher.py:66: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68e7830fe6414dbe917e1208d4a73c03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " epoch  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zc22364\\AppData\\Local\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel_launcher.py:71: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9456d4b7c3e4499ab62f241587933c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zc22364\\AppData\\Local\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel_launcher.py:119: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54817d381d6f45a4a34d4b01ab2f3dff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zc22364\\AppData\\Local\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel_launcher.py:123: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation accuracy 67.31352\n"
     ]
    }
   ],
   "source": [
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    #print('OPTIONS ', options)\n",
    "    # Tuning Parameters:\n",
    "    import easydict\n",
    "    from argparse import ArgumentParser\n",
    "    parser = ArgumentParser()\n",
    "    args = parser.parse_known_args()[0]\n",
    "    #使用 easydict，可以创建一个命名空间对象，该对象具有字典的功能，并允许通过点号操作符来访问和设置键值对。\n",
    "    args = easydict.EasyDict({\n",
    "            \"num_epochs\": 1,\n",
    "            \"batch_size\": 1,\n",
    "            \"lr\": 0.0005,\n",
    "            \"tb_dir\": \"./runs\",\n",
    "            \"final\": False,\n",
    "            \"max_pool\": False,\n",
    "            \"embedding_size\" : 1024,\n",
    "            \"vocab_size\" : 50000,\n",
    "            \"max_length\" : 520,\n",
    "            \"num_heads\" : 1,\n",
    "            \"depth\" : 1,\n",
    "            \"seed\" : 1,\n",
    "            \"lr_warmup\" : 500,\n",
    "            \"gradient_clipping\" : 1.0\n",
    "    })\n",
    "    evaluation = go(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
